{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [data/field.py](https://github.com/pytorch/text/blob/master/torchtext/data/field.py)\n",
    "- [text/test/imdb.py](https://github.com/pytorch/text/blob/master/test/imdb.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Approach 1:\n",
    "# set up fields\n",
    "TEXT = data.Field(lower=True, fix_length=500, batch_first=True, pad_first=True)\n",
    "LABEL = data.Field(sequential=False, pad_token=None, unk_token=None) # vocabularyに反映されてしまうため、unkとpadをNonenに\n",
    "\n",
    "# make splits for data\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print information about the data\n",
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))\n",
    "print('len(test)', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, max_size=4998) # padとunkを考慮 (オプションについてはVocabのコンストラクタを参照)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(TEXT.vocab.itos[11])\n",
    "print(LABEL.vocab.itos[0], LABEL.vocab.itos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make iterator for splits\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=512, device=0, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_iter, 'test': test_iter}\n",
    "dataset_sizes = {'train': len(train), 'test': len(test)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "- 画像と違って1次元なので、Conv1dを用いる\n",
    "- CNNのin_channelsをembeddingの各次元とする\n",
    "- 最終出力は1次元で、[0, 1]の値とする\n",
    "  - よってこれまでのsoftmaxではなくsigmoidにし、loss functionもbinary cross entropyにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "torch.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.emb = nn.Embedding(5000, 32)\n",
    "        self.conv1 = nn.Conv1d(32, 64, kernel_size=5) \n",
    "        self.bn1 = nn.BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
    "        # 500x32 -> 496x64 -> 248x64\n",
    "        self.fc1 = nn.Linear(15872, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.transpose(1, 2) # N x seq_size x embedding_sizeになっているので、N x embedding_size x seq_size に変換する\n",
    "        x = F.dropout(x, training=self.trainig)\n",
    "        x = F.relu(F.max_pool1d(self.conv1(x), 2)) # max_pool1dに\n",
    "        x = self.bn1(x)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.trainig)\n",
    "        x = self.fc2(x)\n",
    "        return self.sig(x)\n",
    "    \n",
    "     def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(1)\n",
    "    model = model.cuda()\n",
    "    \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model_finetuned.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in tqdm(dataloaders[phase]):\n",
    "                # get the inputs\n",
    "                inputs = data.text\n",
    "                labels = data.label.unsqueeze(dim=1).float() # N を Nx1にする\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                preds = outputs.round() # 四捨五入して予測\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds.data == labels.data) # preds.dataに変更\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            # 最も良いモデルの重みを変数に保持\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train_model(model criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove\n",
    "\n",
    "- 自前でEmbeddingを作成してもうまくいかないので、Gloveの重みを活用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Approach 1:\n",
    "# set up fields\n",
    "TEXT = data.Field(lower=True, fix_length=500, batch_first=True, pad_first=True)\n",
    "LABEL = data.Field(sequential=False, pad_token=None, unk_token=None) # vocabularyに反映されてしまうため、unkとpadをNonenに\n",
    "\n",
    "# make splits for data\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, vectors=Glove(name='6B', dim=300))\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', Len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n",
    "TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make iterator for splits\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=512, device=0, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_iter, 'test': test_iter}\n",
    "dataset_sizes = {'train': len(train), 'test': len(test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetGlove(nn.Module):\n",
    "    def __init__(self, glove_weight):\n",
    "        super(NetGlove, self).__init__()\n",
    "        self.emb = nn.Embedding(251639, 300)\n",
    "        self.emb.weight.data.copy_(glove_weight) # Gloveの重みをsetする\n",
    "        self.conv1 = nn.Conv1d(300, 600, kernel_size=5) \n",
    "        self.bn1 = nn.BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True)\n",
    "        # 500x32 -> 496x600 -> 248x600\n",
    "        self.fc1 = nn.Linear(148800, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.transpose(1, 2) # N x seq_size x embedding_sizeになっているので、N x embedding_size x seq_size に変換する\n",
    "        x = F.dropout(x, training=self.trainig)\n",
    "        x = F.relu(F.max_pool1d(self.conv1(x), 2)) # max_pool1dに\n",
    "        x = self.bn1(x)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.trainig)\n",
    "        x = self.fc2(x)\n",
    "        return self.sig(x)\n",
    "    \n",
    "     def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NetGlove(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train_model(model criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "- variable lengthの入力を作成\n",
    "  - padding済みから作成: torch.nn.utils.rnn.pack_padded_sequence\n",
    "    - 引数として別途各lengthを与えることで、どこまでがpadではないか判別\n",
    "  - sequenceから作成: torch.nn.utils.rnn.pack_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetRNN(nn.Module):\n",
    "    def __init__(self, glove_weight):\n",
    "        super(NetRNN, self).__init__()\n",
    "        self.emb = nn.Embedding(251639, 300)\n",
    "        self.emb.weight.data.copy_(glove_weight) # Gloveの重みをsetする\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size=50, num_layers=1, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(50, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.emb(x)\n",
    "        x = x.transpose(0, 1) # N x seq_size x embedding_sizeになっているので、seq_size x N x embedding_size に変換する\n",
    "        # input (seq_len, batch, input_size)\n",
    "        output, (h_n, c_n) = self.lstm(x, hidden)\n",
    "        x = h_n[-1].squeeze(0) # seq_len, batch, hidden_size * num_directions なので[-1]を取る\n",
    "        x = self.fc(x)\n",
    "        return self.sig(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NetRNN(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(1)\n",
    "    model = model.cuda()\n",
    "    \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model_finetuned.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = train_model(model criterion, optimizer, num_epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
