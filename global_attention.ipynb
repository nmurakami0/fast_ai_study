{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/OpenNMT/OpenNMT-py/blob/fc23dfef1ba2f258858b2765d24565266526dc76/onmt/modules/GlobalAttention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global attention takes a matrix and a query vector. It\n",
    "then computes a parameterized convex combination of the matrix\n",
    "based on the input query.\n",
    "        H_1 H_2 H_3 ... H_n\n",
    "          q   q   q       q\n",
    "            |  |   |       |\n",
    "              \\ |   |      /\n",
    "                      .....\n",
    "                  \\   |  /\n",
    "                          a\n",
    "Constructs a unit mapping.\n",
    "    $$(H_1 + H_n, q) => (a)$$\n",
    "    Where H is of `batch x n x dim` and q is of `batch x dim`.\n",
    "    The full def is  $$\\tanh(W_2 [(softmax((W_1 q + b_1) H) H), q] + b_2)$$.:\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GlobalAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(GlobalAttention, self).__init__()\n",
    "        self.linear_in = nn.Linear(dim, dim, bias=False)\n",
    "        self.sm = nn.Softmax()\n",
    "        # concatしたものを入れるので、inputが2倍の大きさ\n",
    "        self.linear_out = nn.Linear(dim*2, dim, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mask = None\n",
    "\n",
    "    def applyMask(self, mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    # contextはencoder_outupts、つまり各tでのhidden state\n",
    "    def forward(self, input, context):\n",
    "        \"\"\"\n",
    "        input: batch x dim\n",
    "        context: batch x sourceL x dim\n",
    "        \"\"\"\n",
    "        targetT = self.linear_in(input).unsqueeze(2)  # batch x dim x 1\n",
    "\n",
    "        # Get attention\n",
    "        # 論文の(8)式のdotバージョン\n",
    "        attn = torch.bmm(context, targetT).squeeze(2)  # batch x sourceL\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        # attention weight\n",
    "        attn = self.sm(attn)\n",
    "        # attention weightを3次元に\n",
    "        attn3 = attn.view(attn.size(0), 1, attn.size(1))  # batch x 1 x sourceL\n",
    "\n",
    "        # weightedContextとinputをLinear layerに掛け、活性化関数tanhに掛ける\n",
    "        weightedContext = torch.bmm(attn3, context).squeeze(1)  # batch x dim\n",
    "        contextCombined = torch.cat((weightedContext, input), 1)\n",
    "\n",
    "        contextOutput = self.tanh(self.linear_out(contextCombined))\n",
    "\n",
    "        # 出力とattention weightを返す\n",
    "        return contextOutput, attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
